{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,BatchNormalization,Lambda,Activation,Input,Flatten,Conv2DTranspose,concatenate,Reshape,LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set some options and read file names and attribute data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#options\n",
    "IMG_DIM = 64\n",
    "DIM_S = 2\n",
    "supervised = True #False:Unsupervised\n",
    "prior_dist = 0 # 0: Isotropic 1: guassian mixture\n",
    "sensitive = 'emotion' #'gender' or 'emotion'\n",
    "\n",
    "# automatically set variables based on selected options\n",
    "DIM_U = 2 if supervised else [IMG_DIM, IMG_DIM, 3]\n",
    "utility = 'emotion' if sensitive == 'gender' else 'gender'\n",
    "prior_type = 'Isotropic' if prior_dist == 0 else 'Mixture'\n",
    "exp_info = f\"s_{sensitive}_u_{utility}_{prior_type}_supervised\" if supervised else f\"s_{sensitive}_{prior_type}_unsupervised\"\n",
    "exp_info += \"_P1\"\n",
    "\n",
    "#Folder and image files\n",
    "generalFolder = './CelebA/'\n",
    "folder = generalFolder + 'img_align_celeba/'\n",
    "allFiles = [folder + \"/\" + f for f in sorted(os.listdir(folder))]#Folder and image files\n",
    "n = len(allFiles)\n",
    "print(\"Number of data : \", n)\n",
    "\n",
    "df = pd.read_csv(generalFolder + 'list_attr_celeba.csv', nrows=n)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading U and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = df['Male'].to_numpy() if(sensitive == 'gender') else df['Smiling'].to_numpy()\n",
    "S[S == -1] = 0\n",
    "\n",
    "if supervised:\n",
    "    U = df['Smiling'].to_numpy() if(sensitive == 'gender') else df['Male'].to_numpy()\n",
    "    U[U == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading partion information for split data into test, train and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_part = pd.read_csv(generalFolder + 'list_eval_partition.txt', header=None, sep=\" \").to_numpy()[:, 1].astype(int)\n",
    "print(eval_part.shape)\n",
    "n_train = sum(eval_part == 0)\n",
    "n_valid = sum(eval_part == 1)\n",
    "n_test  = sum(eval_part == 2)\n",
    "print(n_train, n_valid, n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading image data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot U and S in order to ready them for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.zeros((n_train, IMG_DIM, IMG_DIM, 3))\n",
    "x_valid = np.zeros((n_valid, IMG_DIM, IMG_DIM, 3))\n",
    "x_test = np.zeros((n_test, IMG_DIM, IMG_DIM, 3))\n",
    "\n",
    "print(\"Reading train data:\")\n",
    "for i in range(0,n_train):\n",
    "  if (i+1) % 20000 == 0:\n",
    "    print(f\"{i+1} / {n_train}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_train[i, :, :, :] = img / 255.0\n",
    "\n",
    "print(\"Reading validation data:\")\n",
    "for i in range(n_train,(n_train+n_valid)):\n",
    "  if (i-n_train+1) % 5000 == 0:\n",
    "    print(f\"{i-n_train+1} / {n_valid}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_valid[i-n_train, :, :, :] = img / 255.0\n",
    "    \n",
    "print(\"Reading test data:\")\n",
    "for i in range((n_train+n_valid),n):\n",
    "  if (i-(n_train+n_valid)+1) % 5000 == 0:\n",
    "    print(f\"{i-(n_train+n_valid)+1} / {n_test}\")\n",
    "  img = cv2.imread(allFiles[i])\n",
    "  img = cv2.resize(img,(IMG_DIM,IMG_DIM),interpolation = cv2.INTER_AREA)\n",
    "  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "  x_test[i-(n_train+n_valid), :, :, :] = img / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, num_labels):\n",
    "    num_labels_data = labels.shape[0]\n",
    "    one_hot_encoding = np.zeros((num_labels_data,num_labels))\n",
    "    one_hot_encoding[np.arange(num_labels_data),labels] = 1\n",
    "    one_hot_encoding = np.reshape(one_hot_encoding, [-1, num_labels])\n",
    "    return one_hot_encoding\n",
    "\n",
    "s_train = one_hot(S[:n_train], DIM_S).astype(np.float32)\n",
    "s_valid = one_hot(S[n_train:(n_train+n_valid)], DIM_S).astype(np.float32)\n",
    "s_test = one_hot(S[(n_train+n_valid):], DIM_S).astype(np.float32)\n",
    "\n",
    "\n",
    "if not supervised:\n",
    "    u_train = x_train\n",
    "    u_valid = x_valid\n",
    "    u_test = x_test\n",
    "else:\n",
    "    u_train = one_hot(U[:n_train], DIM_U).astype(np.float32)\n",
    "    u_valid = one_hot(U[n_train:(n_train+n_valid)], DIM_U).astype(np.float32)\n",
    "    u_test = one_hot(U[(n_train+n_valid):], DIM_U).astype(np.float32)\n",
    "\n",
    "\n",
    "print(s_train.shape, s_valid.shape, s_test.shape)\n",
    "print(u_train.shape, u_valid.shape, u_test.shape)\n",
    "print(DIM_U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(DIM_Z, input_x):\n",
    "    stride = 2\n",
    "#     input_x = Input( shape = [IMG_DIM,IMG_DIM,3], name=\"x\" )\n",
    "\n",
    "    #first hidden layer\n",
    "    x = Conv2D(16, 3, strides=stride, padding=\"same\", name=\"enc_h1\")(input_x)\n",
    "    x = BatchNormalization(name=\"enc_h1_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h1_activation\")(x)\n",
    "    #second hidden layer\n",
    "    x = Conv2D(32, 3, strides=stride, padding=\"same\", name=\"enc_h2\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h2_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h2_activation\")(x)\n",
    "    #third hidden layer\n",
    "    x = Conv2D(64, 3, strides=stride, padding=\"same\", name=\"enc_h3\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h3_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h3_activation\")(x)\n",
    "    #forth hidden layer\n",
    "    x = Conv2D(128, 3, strides=stride, padding=\"same\", name=\"enc_h4\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h4_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h4_activation\")(x)\n",
    "    #fifth hidden layer\n",
    "    x = Conv2D(256, 3, strides=stride, padding=\"same\", name=\"enc_h5\")(x)\n",
    "    x = BatchNormalization(name=\"enc_h5_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"enc_h5_activation\")(x)\n",
    "    \n",
    "    shape = K.int_shape(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(DIM_Z*4, name=\"enc_dense_1\")(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('tanh')(x)\n",
    "\n",
    "    z_mean = Dense(DIM_Z, name=\"z_mean\")(x)\n",
    "    z_log_sigma_sq = Dense(DIM_Z, name=\"z_sigma\")(x)\n",
    "    z = Lambda(sampling, output_shape=DIM_Z, name='lambda_z')([z_mean, z_log_sigma_sq])\n",
    "    \n",
    "#     prior_loss = K.mean(-0.5 * K.sum(1 + z_log_sigma_sq - K.square(z_mean) - K.exp(z_log_sigma_sq), axis=-1))\n",
    "    \n",
    "    encoder = Model(input_x, z, name = \"Encoder\")\n",
    "#     encoder.add_loss((alpha+beta) * prior_loss)\n",
    "    \n",
    "    return (encoder, z_mean, z_log_sigma_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder(DIM_Z):\n",
    "    stride = 2\n",
    "    input_z = Input(shape = (DIM_Z,), name=\"z_encoder\")\n",
    "    input_s = Input( shape = (DIM_S,), name=\"s\")\n",
    "\n",
    "    z_with_s = concatenate([input_z, input_s], name=\"concat_layer\")\n",
    "\n",
    "    x = Dense(2*2*256, name=\"dec_dense_1\")(z_with_s)\n",
    "    x = BatchNormalization(name=\"dec_dense_1_normalized\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    #Unflatten\n",
    "    x = Reshape((2,2,256), name=\"unflatten1\")(x)\n",
    "\n",
    "    #first hidden layer\n",
    "    x = Conv2DTranspose(128, 3, strides=stride, padding=\"same\", name=\"dec_h1\")(x)\n",
    "    x = BatchNormalization(name=\"dec_h1_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"dec_h1_activation\")(x)\n",
    "    \n",
    "    #second hidden layer\n",
    "    x = Conv2DTranspose(64, 3, strides=stride, padding=\"same\", name=\"dec_h2\")(x)\n",
    "    x = BatchNormalization(name=\"dec_h2_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"dec_h2_activation\")(x)\n",
    "\n",
    "    #third hidden layer\n",
    "    x = Conv2DTranspose(32, 3, strides=stride, padding=\"same\", name=\"dec_h3\")(x)\n",
    "    x = BatchNormalization(name=\"dec_h3_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"dec_h3_activation\")(x)\n",
    "\n",
    "    #forth hidden layer\n",
    "    x = Conv2DTranspose(16, 3, strides=stride, padding=\"same\", name=\"dec_h4\")(x)\n",
    "    x = BatchNormalization(name=\"dec_h4_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"dec_h4_activation\")(x)\n",
    "    \n",
    "    #fifth hidden layer\n",
    "    x = Conv2DTranspose(8, 3, strides=stride, padding=\"same\", name=\"dec_h5\")(x)\n",
    "    x = BatchNormalization(name=\"dec_h5_normalized\")(x)\n",
    "    x = Activation(LeakyReLU(), name=\"dec_h5_activation\")(x)\n",
    "    \n",
    "    x = Conv2D(3, 3, strides=1, padding=\"same\", name=\"dec_h6\")(x)\n",
    "    x_hat = Activation(\"sigmoid\", name=\"x_hat\")(x)\n",
    "    \n",
    "    decoder = Model([input_z, input_s], x_hat, name = \"Uncertainty_Decoder\")\n",
    "    return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    def get_utility_model(DIM_Z):\n",
    "        model = Sequential(name=\"Utility_Decoder_supervised\")\n",
    "        model.add(Dense(DIM_Z, input_dim=DIM_Z))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(DIM_U, activation='softmax', name=\"u_hat\"))\n",
    "        return model\n",
    "else:\n",
    "    def get_utility_model(DIM_Z):\n",
    "        stride = 2\n",
    "        model = Sequential(name=\"Utility_Decoder_unsupervised\")\n",
    "        model.add(Dense(2*2*256, input_dim=DIM_Z))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Reshape((2,2,256)))\n",
    "        \n",
    "        model.add(Conv2DTranspose(128, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(64, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(32, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(16, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2DTranspose(8, 3, strides=stride, padding=\"same\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        \n",
    "        model.add(Conv2D(3, 3, strides=1, padding=\"same\", activation=\"sigmoid\", name=\"u_hat\"))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z_discriminator(DIM_Z):\n",
    "    model = Sequential(name=\"Latent_Space_Discriminator\")\n",
    "    \n",
    "    model.add(Dense(512, input_dim=DIM_Z))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_utility_discriminator():\n",
    "    model = Sequential(name=\"Attribute_Class_Discriminator\")\n",
    "\n",
    "    model.add(Dense(DIM_U * 8, input_dim=DIM_U))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(DIM_U * 8,))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_visible_space_discriminator():\n",
    "    stride = 2\n",
    "    model = Sequential(name=\"Visible_Space_Discriminator\")\n",
    "    \n",
    "    model.add(Input(shape = [IMG_DIM,IMG_DIM,3]))\n",
    "    \n",
    "    model.add(Conv2D(16, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(32, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(64, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    \n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prior_generator(DIM_Z, noise_dim=100):\n",
    "    model = Sequential(name=\"Prior_Distribution_Generator\")\n",
    "    \n",
    "    model.add(Dense(noise_dim*2, input_dim=noise_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(noise_dim))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(DIM_Z))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train a model for evaluating S performance\n",
    "The following two block cells are **not** part of the framework, it's an evaluator for sensitivity which is required for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s_eval_model():\n",
    "    stride = 2\n",
    "    model = Sequential(name=\"CNN_Model_sensitivity_evaluator\")\n",
    "    model.add(Input(shape = [IMG_DIM,IMG_DIM,3]))\n",
    "\n",
    "    model.add(Conv2D(32, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "\n",
    "    model.add(Conv2D(64, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "\n",
    "    model.add(Conv2D(128, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "\n",
    "    model.add(Conv2D(256, 3, strides=stride, padding=\"same\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(LeakyReLU(alpha=0.2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1024))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(\"relu\"))\n",
    "\n",
    "    model.add(Dense(DIM_S, activation=\"softmax\", name='s_hat'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_eval_model = get_s_eval_model()\n",
    "s_eval_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005), loss='mse')\n",
    "\n",
    "force_train = False\n",
    "\n",
    "if not os.path.exists(f\"saved_models/celeba_eval_model_s_{sensitive}.h5\") or force_train:\n",
    "    print(\"Training sensitive attribute evaluator\")\n",
    "    history = s_eval_model.fit(x_train, s_train, validation_data=(x_valid, s_valid), batch_size=2048, epochs=100, shuffle=True, verbose=2)\n",
    "    s_eval_model.save_weights(f\"saved_models/celeba_eval_model_s_{sensitive}.h5\")\n",
    "else:\n",
    "    print(\"Loading sensitive attribute evaluator from file\")\n",
    "    s_eval_model.load_weights(f\"saved_models/celeba_eval_model_s_{sensitive}.h5\")\n",
    "\n",
    "## Evaluating evaluator!\n",
    "s_hat_test = s_eval_model.predict(x_test)\n",
    "print(f\"Evaluator accuracy = {(sum(np.argmax(s_hat_test,axis=-1)==np.argmax(s_test,axis=-1)) / s_test.shape[0]) * 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all togther - define CLUB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_gen_enabled = False #if False, the algorithm sample prior from Normal Distribution\n",
    "\n",
    "## utility and reconstruction losses\n",
    "if supervised:\n",
    "    def loss_u(u_true, u_pred):\n",
    "        return K.mean(K.sum(K.square(u_true-u_pred), axis=-1))\n",
    "else:\n",
    "    def loss_u(u_true, u_pred):\n",
    "        return K.mean(K.sum(K.square(u_true-u_pred), axis=(1,2,3)))\n",
    "\n",
    "def loss_x(alpha):\n",
    "    def loss(inp_x, out_x):\n",
    "        return alpha * K.mean(K.sum(K.square(inp_x-out_x), axis=(1,2,3)))\n",
    "    return loss\n",
    "\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "# Weighted cross-entropy loss\n",
    "def loss_wce(coef):\n",
    "    def loss(y, y_pred):\n",
    "         return coef * bce(y, y_pred)\n",
    "    return loss\n",
    "\n",
    "def get_full_model(dim_z, alpha, beta, learning_rate=0.0001, dim_noise = 100):\n",
    "    ########## Inputs\n",
    "    input_x = Input( shape=[IMG_DIM,IMG_DIM,3], name=\"x\" )\n",
    "    input_s = Input( shape = (DIM_S,), name=\"s\" )\n",
    "    input_z = Input( shape = (dim_z,), name=\"z\" )\n",
    "    input_noise = Input( shape = (dim_noise,), name=\"Noise\")\n",
    "\n",
    "    ########## Define AE: Encoder, Utility Decoder and Uncertainty Decoder\n",
    "    encoder,z_mean,z_log_sigma_sq = get_encoder(dim_z, input_x)\n",
    "    uncertainty_decoder = get_decoder(dim_z)\n",
    "    utility_decoder = get_utility_model(dim_z)\n",
    "\n",
    "    z = encoder(input_x)\n",
    "    x_hat = uncertainty_decoder([z, input_s])\n",
    "    u_hat = utility_decoder(z)\n",
    "\n",
    "    autoencoder = Model([input_x, input_s], [x_hat, u_hat], name=\"CLUB_Autoencoder\")\n",
    "    prior_loss = (alpha+beta) * K.mean(-0.5 * K.sum(1 + z_log_sigma_sq - K.square(z_mean) - K.exp(z_log_sigma_sq), axis=-1))\n",
    "    prior_loss = tf.identity(prior_loss, name=\"kl_loss\")\n",
    "    autoencoder.add_loss(prior_loss)\n",
    "    autoencoder.compile(loss=[loss_x(alpha), loss_u], optimizer=tf.keras.optimizers.Adam(lr=learning_rate*5))\n",
    "\n",
    "    ########## Define Latent Space Discriminator\n",
    "    z_discriminator = get_z_discriminator(dim_z)\n",
    "    z_discriminator.compile(loss=loss_wce(0.1*(alpha+beta)), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    #z_discriminator should train separately\n",
    "    z_discriminator.trainable = False\n",
    "    prior_generator = get_prior_generator(dim_z, dim_noise)\n",
    "    prior_gen_zdiscriminator = Model(input_noise, z_discriminator(prior_generator(input_noise)), name=\"CLUB_generator_zdiscriminator\")\n",
    "    prior_gen_zdiscriminator.compile(loss=loss_wce(-0.1*(alpha+beta)), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    encoder_zdiscriminator = Model(input_x, z_discriminator(encoder(input_x)), name=\"CLUB_encoder_zdiscriminator\")\n",
    "    encoder_zdiscriminator.compile(loss=loss_wce(-0.1*(alpha+beta)), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    u_dircriminator = get_utility_discriminator() if supervised else get_visible_space_discriminator()\n",
    "    u_dircriminator.compile(loss=loss_wce(0.1), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "\n",
    "    ##########\n",
    "    #u_dircriminator should train separately\n",
    "    u_dircriminator.trainable = False\n",
    "\n",
    "    if z_gen_enabled:    \n",
    "        decoder_udiscriminator = Model(input_noise, u_dircriminator(utility_decoder(prior_generator(input_noise))), name=\"CLUB_decoder_discriminator\")\n",
    "    else:\n",
    "        decoder_udiscriminator = Model(input_z, u_dircriminator(utility_decoder(input_z)), name=\"CLUB_decoder_discriminator\")\n",
    "\n",
    "    decoder_udiscriminator.compile(loss=loss_wce(-0.1), optimizer=tf.keras.optimizers.Adam(lr=learning_rate))\n",
    "    \n",
    "    return encoder,uncertainty_decoder,utility_decoder,autoencoder,z_discriminator,prior_generator,prior_gen_zdiscriminator,encoder_zdiscriminator,u_dircriminator,decoder_udiscriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test\n",
    "# encoder,uncertainty_decoder,utility_decoder,autoencoder,z_discriminator,prior_generator,prior_gen_zdiscriminator,encoder_zdiscriminator,u_dircriminator,decoder_udiscriminator = get_full_model(32,0.1,0.1)\n",
    "# tf.keras.utils.plot_model(decoder_udiscriminator, show_shapes=True)\n",
    "# tf.keras.utils.plot_model(autoencoder, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-training implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train_ae(dim_z, alpha, beta, force_train=False, max_itr=100, batch_size=1024, verbose=2):\n",
    "    info_str = f\"d_{dim_z}_beta_{beta}_alpha_{alpha}_{exp_info}\"\n",
    "    if not os.path.exists(f\"saved_models/celeba_pretrain_{info_str}.h5\") or force_train:\n",
    "        print(f\"Pre-Training with {info_str}\")\n",
    "        autoencoder.fit([x_train, s_train], [x_train, u_train], validation_data=([x_valid, s_valid], [x_valid, u_valid]), batch_size=batch_size, epochs=max_itr, shuffle=True, verbose=verbose)\n",
    "        autoencoder.save_weights(f\"saved_models/celeba_pretrain_{info_str}.h5\")\n",
    "    else:\n",
    "        print(f\"Loading model from file with {info_str}\")\n",
    "        autoencoder.load_weights(f\"saved_models/celeba_pretrain_{info_str}.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full model training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set prior distribution function based on user config (The following cell will be ignored if the prior-generator network enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.sampler import gaussian, gaussian_mixture\n",
    "\n",
    "def sample_prior(latent_dim, batch_size):\n",
    "    if prior_dist == 0:  \n",
    "        return gaussian(batch_size, latent_dim)\n",
    "    elif prior_dist == 1:\n",
    "        return gaussian_mixture(batch_size, latent_dim, num_labels=min(10, DIM_U))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## customized training loop for block-wised algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_train(dim_z, max_itr=1000, batch_size=1024, z_disc_enabled=True,u_disc_enabled=True,verbose=0, dim_noise=100):\n",
    "    ones = np.ones((batch_size, 1))\n",
    "    zeros = np.zeros((batch_size, 1))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for epoch in range(max_itr):\n",
    "        start_time_epoch = time.time()\n",
    "        # Select a random batch of images\n",
    "        idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "        x = x_train[idx]\n",
    "        s = s_train[idx]\n",
    "        u = u_train[idx]\n",
    "\n",
    "        # ---------------------\n",
    "        #  1- Train the Encoder, Utility Decoder, Uncertainty Decoder\n",
    "        # ---------------------\n",
    "        ae_loss = autoencoder.train_on_batch([x, s], [x, u])\n",
    "\n",
    "        if z_disc_enabled:\n",
    "        # ---------------------\n",
    "        #  2- Train the Latent Space Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                latent_prior = prior_generator(noise)\n",
    "            else:\n",
    "                latent_prior = sample_prior(dim_z, batch_size)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            x = x_train[idx]\n",
    "            latent_enc = encoder(x)\n",
    "\n",
    "            d_loss_prior = z_discriminator.train_on_batch(latent_prior, zeros)\n",
    "            d_loss_enc = z_discriminator.train_on_batch(latent_enc, ones)\n",
    "            dz_loss = d_loss_prior + d_loss_enc\n",
    "\n",
    "            # ---------------------\n",
    "            # 3- Train the Encoder and Prior Distribution Generator Adversarially\n",
    "            # ---------------------\n",
    "            prior_loss = 0.\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                prior_loss = prior_gen_zdiscriminator.train_on_batch(noise, zeros)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            x = x_train[idx]\n",
    "            edz_loss = encoder_zdiscriminator.train_on_batch(x, ones)\n",
    "        else:\n",
    "            dz_loss = 0.\n",
    "            prior_loss = 0.\n",
    "            edz_loss = 0.\n",
    "\n",
    "        if u_disc_enabled:\n",
    "        # ---------------------\n",
    "        #  4- Train Visible_Space/Attribute_Class Discriminator \n",
    "        # ---------------------\n",
    "\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                latent = prior_generator(noise)\n",
    "            else:\n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "            u_dec = utility_decoder(latent)\n",
    "\n",
    "            idx = np.random.randint(0, x_train.shape[0], batch_size)\n",
    "            u = u_train[idx]\n",
    "\n",
    "            d_loss_real = u_dircriminator.train_on_batch(u, ones)\n",
    "            d_loss_fake = u_dircriminator.train_on_batch(u_dec, zeros)\n",
    "            du_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # ---------------------\n",
    "        #  5- Train the Prior Distribution Generator and Utility Decoder Adversarially\n",
    "        # ---------------------\n",
    "            if z_gen_enabled:\n",
    "                noise = np.random.uniform(-1.0, 1.0, size=[batch_size, dim_noise])\n",
    "                gdu_loss = decoder_udiscriminator.train_on_batch(noise, zeros)\n",
    "            else:    \n",
    "                latent = sample_prior(dim_z, batch_size)\n",
    "                gdu_loss = decoder_udiscriminator.train_on_batch(latent, zeros)\n",
    "        else:\n",
    "            gdu_loss = 0\n",
    "            du_loss = 0\n",
    "\n",
    "        # ---------------------\n",
    "        #  Print stats info\n",
    "        # ---------------------\n",
    "        if verbose != 0 and epoch % 50 == 0:\n",
    "            epoch_time = (time.time() - start_time_epoch)\n",
    "            print(f\"{epoch}, mse:{ae_loss[1]:.4f}, u:{ae_loss[2]:.4f}, dz:{dz_loss:.4f}, edz:{edz_loss:.4f}, prior:{prior_loss:.4f}, du:{du_loss:.4f}, gdu:{gdu_loss:.4f}\")\n",
    "            print(f\"Each epoch execution time: {(time.time() - start_time_epoch):.5f} seconds\")\n",
    "\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print(f\"Total Execution Time: {total_time:.4f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following function used for showing the uncertainty-decoder outputs (reconsterction) with different S values on the list of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figures(club, dim_z, alpha, beta):\n",
    "    info_str = f\"d_{dim_z}_beta_{beta}_alpha_{alpha}_{exp_info}\"\n",
    "    \n",
    "    n_plot_samps = 8\n",
    "    ind_list = [3, 2, 1, 18, 4 ,15, 11, 0]\n",
    "\n",
    "    s_hat = s_eval_model.predict(x_test[ind_list])[:, 0]\n",
    "\n",
    "    rev_s = np.ones((n_plot_samps,2)) - s_test[ind_list]\n",
    "\n",
    "    x_hat_org, *_ = club.predict([x_test[ind_list], s_test[ind_list] ])\n",
    "    s_hat = np.append(s_hat, s_eval_model.predict(x_hat_org)[:, 0])\n",
    "\n",
    "    x_hat_rev, *_ = club.predict([x_test[ind_list], rev_s ])\n",
    "    s_hat = np.append(s_hat, s_eval_model.predict(x_hat_rev)[:, 0])\n",
    "\n",
    "    x_hat_no_s, *_ = club.predict([x_test[ind_list], np.zeros((n_plot_samps, DIM_S)) ])\n",
    "    s_hat = np.append(s_hat, s_eval_model.predict(x_hat_no_s)[:, 0])\n",
    "\n",
    "    x_hat_all_s, *_ = club.predict([x_test[ind_list], np.ones((n_plot_samps, DIM_S)) ])\n",
    "    s_hat = np.append(s_hat, s_eval_model.predict(x_hat_all_s)[:, 0])\n",
    "    \n",
    "    label = \"Male\" if sensitive == \"gender\" else \"Smilling\"\n",
    "    s_hat = [f\"P({label}) = \" + str(\"{:.2f}\".format(1-s)) for s in s_hat]\n",
    "\n",
    "    from utils.pics_tools import plot_image_grid_with_label\n",
    "    fig = plot_image_grid_with_label(np.concatenate([x_test[ind_list], x_hat_org, x_hat_rev, x_hat_no_s, x_hat_all_s], axis=0), [IMG_DIM, IMG_DIM, 3], (5,n_plot_samps), s_hat)\n",
    "    fig.set_size_inches(17, 14)\n",
    "    plt.subplots_adjust(wspace=0, hspace=0.01)\n",
    "    plt.savefig(f'./saved_figures_celeba/celeba_{info_str}.svg', format='svg')\n",
    "    plt.savefig(f'./saved_figures_celeba/celeba_{info_str}.png', format='png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop of experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.mine import MINE\n",
    "alpha_list = [0.0001, 0.1, 0.9]\n",
    "beta_list = [0.0001, 0.1, 0.3, 0.5, 0.8, 0.95]\n",
    "DIM_Z = [64, 128]\n",
    "\n",
    "util_acc_tr = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "util_acc_ts = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "util_acc_va = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_mae_tr = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_mae_ts = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_mae_tr_rev = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_mae_ts_rev = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "\n",
    "sens_acc_tr = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_acc_ts = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_acc_tr_rev = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "sens_acc_ts_rev = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "\n",
    "\n",
    "mi_u_ts = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "mi_u_tr = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "mi_s_ts = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "mi_s_tr = np.zeros((len(DIM_Z),len(alpha_list),len(beta_list)))\n",
    "\n",
    "s_test_rev = np.ones((n_test,2)) - s_test\n",
    "s_train_rev = np.ones((n_train,2)) - s_train\n",
    "\n",
    "for i, dimz in enumerate(DIM_Z):\n",
    "    for j, alpha in enumerate(alpha_list):\n",
    "        for k, beta in enumerate(beta_list):\n",
    "            #Pre training\n",
    "            encoder,uncertainty_decoder,utility_decoder,autoencoder,z_discriminator,prior_generator,prior_gen_zdiscriminator,encoder_zdiscriminator,u_dircriminator,decoder_udiscriminator = get_full_model(dimz, alpha, beta)\n",
    "            pre_train_ae(dimz, alpha, beta, force_train=False, batch_size=1024, max_itr=50, verbose=0)\n",
    "            \n",
    "            print(\"Full model training\")\n",
    "            main_train(dimz, max_itr=100, batch_size=1024, verbose=1)\n",
    "\n",
    "            print(\"Evaluate performance of U\")\n",
    "            z_test = encoder.predict(x_test)\n",
    "            z_train = encoder.predict(x_train)\n",
    "            z_valid = encoder.predict(x_valid)\n",
    "            \n",
    "            u_test_hat = utility_decoder.predict(z_test)\n",
    "            u_train_hat = utility_decoder.predict(z_train)\n",
    "            u_valid_hat = utility_decoder.predict(z_valid)\n",
    "            if supervised:\n",
    "                util_acc_ts[i][j][k] = np.mean(np.argmax(u_test_hat,axis=1)==np.argmax(u_test,axis=1)) * 100\n",
    "                util_acc_tr[i][j][k] = np.mean(np.argmax(u_train_hat,axis=1)==np.argmax(u_train,axis=1)) * 100\n",
    "                util_acc_va[i][j][k] = np.mean(np.argmax(u_valid_hat,axis=1)==np.argmax(u_valid,axis=1)) * 100\n",
    "            else:\n",
    "                util_acc_ts[i][j][k] = np.mean(np.sum(np.square(u_test_hat-u_test), axis=(1,2,3)))\n",
    "                util_acc_tr[i][j][k] = np.mean(np.sum(np.square(u_train_hat-u_train), axis=(1,2,3)))\n",
    "                util_acc_va[i][j][k] = np.mean(np.sum(np.square(u_valid_hat-u_valid), axis=(1,2,3)))\n",
    "            \n",
    "            save_figures(autoencoder, dimz, alpha, beta)\n",
    "    \n",
    "            \n",
    "            print(\"Evaluate performance of S\")\n",
    "            # orginal\n",
    "            x_test_hat = uncertainty_decoder.predict([z_test, s_test])\n",
    "            s_test_hat = s_eval_model(x_test_hat)\n",
    "            sens_mae_ts[i][j][k] = np.mean(np.abs(s_test_hat - s_test))\n",
    "            sens_acc_ts[i][j][k] = np.mean(np.argmax(s_test_hat,axis=1)==np.argmax(s_test,axis=1)) * 100\n",
    "            \n",
    "#             x_train_hat = uncertainty_decoder.predict([z_train, s_train])\n",
    "#             s_train_hat = s_eval_model(x_train_hat)\n",
    "#             sens_mae_tr[i][j][k] = np.mean(np.abs(s_train_hat - s_train))\n",
    "#             sens_acc_tr[i][j][k] = np.mean(np.argmax(s_train_hat,axis=1)==np.argmax(s_train,axis=1)) * 100\n",
    "            \n",
    "            # reverse\n",
    "            x_test_hat = uncertainty_decoder.predict([z_test, s_test_rev])\n",
    "            s_test_hat = s_eval_model(x_test_hat)\n",
    "            sens_mae_ts_rev[i][j][k] = np.mean(np.abs(s_test_hat - s_test_rev))\n",
    "            sens_acc_ts_rev[i][j][k] = np.mean(np.argmax(s_test_hat,axis=1)==np.argmax(s_test_rev,axis=1)) * 100\n",
    "            \n",
    "#             x_train_hat = uncertainty_decoder.predict([z_train, s_train_rev])\n",
    "#             s_train_hat = s_eval_model(x_train_hat)\n",
    "#             sens_mae_tr_rev[i][j][k] = np.mean(np.abs(s_train_hat - s_train_rev))\n",
    "#             sens_acc_tr_rev[i][j][k] = np.mean(np.argmax(s_train_hat,axis=1)==np.argmax(s_train_rev,axis=1)) * 100\n",
    "            if supervised:\n",
    "                print(\"Evaluate Mutual Information I(Z;U)\")            \n",
    "                mine = MINE(x_dim=dimz, y_dim=DIM_U)\n",
    "                _, mi_u_ts[i][j][k] = mine.fit(z_test, u_test, epochs=250, batch_size=1024, verbose=0)\n",
    "                _, mi_u_tr[i][j][k] = mine.fit(z_train, u_train, epochs=250, batch_size=1024, verbose=0)\n",
    "            \n",
    "            print(\"Evaluate Mutual Information I(Z;S)\")\n",
    "            mine = MINE(x_dim=dimz, y_dim=DIM_S)\n",
    "            _, mi_s_ts[i][j][k] = mine.fit(z_test, s_test, epochs=250, batch_size=1024, verbose=0)\n",
    "            _, mi_s_tr[i][j][k] = mine.fit(z_train, s_train, epochs=250, batch_size=1024, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results\n",
    "## Plot accuracy U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(util_acc_ts.shape[0]):\n",
    "    plt.figure(figsize=(15,8))\n",
    "#     if supervised:\n",
    "#         plt.title(r'Utility Attribute accuracy ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "#     else:\n",
    "#         plt.title(r'Utility attribute MSE ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "    for j in range(util_acc_ts.shape[1]):\n",
    "        if supervised:\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att Acc., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "        else:\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att MSE., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "            \n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.xticks(list(range(len(beta_list))), beta_list, fontsize=18, rotation=90)\n",
    "    plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "    if supervised:\n",
    "        plt.ylabel(r'Accuracy on $\\mathbf{U}$', fontsize=16)\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "    else:\n",
    "        plt.ylabel(r'MSE of $\\mathbf{U}$', fontsize=16)\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mse_u_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mse_u_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy U (for each z and alpha show and save one figure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(util_acc_ts.shape[0]):\n",
    "    for j in range(util_acc_ts.shape[1]):\n",
    "        plt.figure(figsize=(15,8))\n",
    "        if supervised:\n",
    "#             plt.title(r'Utility Attribute accuracy ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att Acc., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "            plt.main_train(util_acc_tr[i][j], label=r'Utility Att Acc., Train, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[1], linewidth=3)\n",
    "            plt.plot(util_acc_va[i][j], label=r'Utility Att Acc., Validation, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[3], linewidth=3)\n",
    "        else:\n",
    "#             plt.title(r'Utility attribute MSE ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "            plt.plot(util_acc_ts[i][j], label=r'Utility Att MSE., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "            plt.plot(util_acc_tr[i][j], label=r'Utility Att MSE., Train, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[1], linewidth=3)\n",
    "            plt.plot(util_acc_va[i][j], label=r'Utility Att MSE., Validation, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[3], linewidth=3)\n",
    "            \n",
    "        plt.legend(prop={'size': 16})\n",
    "        plt.grid()\n",
    "        plt.xticks(list(range(len(beta_list))), beta_list, fontsize=18, rotation=90)\n",
    "        plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "        if supervised:\n",
    "            plt.ylabel(r'Accuracy on $\\mathbf{U}$', fontsize=16)\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_alpha_{alpha_list[j]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_acc_d_{DIM_Z[i]}_alpha_{alpha_list[j]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "        else:\n",
    "            plt.ylabel(r'MSE of $\\mathbf{U}$', fontsize=16)\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_mse_u_d_{DIM_Z[i]}_alpha_{alpha_list[j]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "            plt.savefig(f'./saved_figures/chart_celeba_mse_u_d_{DIM_Z[i]}_alpha_{alpha_list[j]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot accuracy S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(sens_acc_ts_rev.shape[0]):\n",
    "    plt.figure(figsize=(15,8))\n",
    "#     plt.title(r'Sensitive attribute accuracy ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "    for j in range(sens_acc_ts_rev.shape[1]):\n",
    "        plt.plot(sens_acc_ts[i][j], label=r'Sensitive Att. Acc., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "        \n",
    "        plt.plot(sens_acc_ts_rev[i][j], label=r'Sensitive Att. Rev Acc., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.xticks(list(range(len(beta_list))), beta_list, fontsize=18, rotation=90)\n",
    "    plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "    plt.ylabel(r'Accuracy on $\\mathbf{S}$', fontsize=16)\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_acc_s_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_acc_s_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Mean absolute error (MAE) on all sensitivity states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "for i in range(sens_mae_ts.shape[0]):\n",
    "    plt.figure(figsize=(15,8))\n",
    "#     plt.title(r'MAE of S ' + f'{exp_info}_d={DIM_Z[i]}', fontsize=16)\n",
    "    for j in range(sens_mae_ts.shape[1]):\n",
    "        plt.plot(sens_mae_ts[i][j], label=r'Sensitive Att. MAE., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "        \n",
    "        plt.plot(sens_mae_ts_rev[i][j], label=r'Sensitivity Rev MAE., Test, $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0], linewidth=2)\n",
    "\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.xticks(list(range(len(beta_list))), beta_list, fontsize=18, rotation=90)\n",
    "    plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "    plt.ylabel(r'Mean Absolute Error (MAE) on $\\mathbf{S}$', fontsize=16)\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mae_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mae_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot mutual information between Z and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.rcParams['figure.dpi'] = 300\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "colors = ['b', 'r', 'g', 'c']\n",
    "for i in range(mi_s_ts.shape[0]):\n",
    "    plt.figure(figsize=(15,8))\n",
    "#     plt.title(r'Mutual Information between $\\mathbf{S}$ and $\\mathbf{Z}$' + f' - dz_{DIM_Z[i]}_{exp_info}', fontsize=20)\n",
    "    for j in range(mi_s_ts.shape[1]):\n",
    "        plt.plot(mi_s_ts[i][j], label=r'Test, $d_z=$' + f'{DIM_Z[i]},' + r'$\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0] ,linewidth=3)\n",
    "#         plt.plot(mi_s_tr[i][j], label=r'Train, $d_z=$' + f'{DIM_Z[i]},' + r'$\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[1], c=colors[j%len(colors)] ,linewidth=3)\n",
    "        \n",
    "    # plt.xscale('linear')\n",
    "    plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "    plt.xticks(list(range(len(beta_list))), beta_list, rotation=90)\n",
    "\n",
    "    plt.ylabel(r'I($\\mathbf{S}$;$\\mathbf{Z}$)', fontsize=24)\n",
    "    plt.legend(prop={'size': 16})\n",
    "    plt.grid()\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mi_zs_dz_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "    plt.savefig(f'./saved_figures/chart_celeba_mi_zs_dz_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot mutual information between Z and U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if supervised:\n",
    "    plt.rcParams.update({'font.size': 16})\n",
    "    plt.rcParams['figure.dpi'] = 300\n",
    "\n",
    "    for i in range(mi_u_ts.shape[0]):\n",
    "        plt.figure(figsize=(15,8))\n",
    "#         plt.title(r'Mutual Information between $\\mathbf{U}$ and $\\mathbf{Z}$' + f' - {exp_info}_dz_{DIM_Z[i]}', fontsize=20)\n",
    "        linestyles = ['-', '--', '-.', ':']\n",
    "        colors = ['b', 'r', 'g', 'c']\n",
    "        for j in range(mi_u_ts.shape[1]):\n",
    "            plt.plot(mi_u_ts[i][j], label=r'Test, $d_z=$' + f'{DIM_Z[i]}' + r'$, \\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[0] ,linewidth=3)\n",
    "    #         plt.plot(mi_u_tr[i][j], label=r'Train, $d_z=$' + f'{DIM_Z[i]}' + r', $\\alpha=$' + f\"{alpha_list[j]}\", linestyle=linestyles[1], c=colors[j%len(colors)] ,linewidth=3)\n",
    "\n",
    "        # plt.xscale('linear')\n",
    "        plt.xlabel(r'$\\beta$', fontsize=24)\n",
    "        plt.xticks(list(range(len(beta_list))), beta_list, rotation=90)\n",
    "\n",
    "        plt.ylabel(r'I($\\mathbf{U}$;$\\mathbf{Z}$)', fontsize=24)\n",
    "        plt.legend(prop={'size': 16})\n",
    "        plt.grid()\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mi_zu_d_{DIM_Z[i]}_{exp_info}.svg', format='svg', bbox_inches='tight')\n",
    "        plt.savefig(f'./saved_figures/chart_celeba_mi_zu_d_{DIM_Z[i]}_{exp_info}.png', format='png', bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results values to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "sio.savemat(f'./saved_data/acc_celeba_{exp_info}.mat', {'util_acc_ts':util_acc_ts, 'util_acc_tr':util_acc_tr, 'util_acc_va':util_acc_va, 'sens_mae_ts':sens_mae_ts, 'sens_mae_tr':sens_mae_tr, 'sens_mae_ts_rev':sens_mae_ts_rev, 'sens_mae_tr_rev':sens_mae_tr_rev,'sens_acc_tr':sens_acc_tr,'sens_acc_ts':sens_acc_ts,'sens_acc_tr_rev':sens_acc_tr_rev,'sens_acc_ts_rev':sens_acc_ts_rev})\n",
    "sio.savemat(f'./saved_data/mi_celeba_{exp_info}.mat', {'mi_s_ts':mi_s_ts, 'mi_s_tr':mi_s_tr, 'mi_u_ts':mi_u_ts, 'mi_u_tr':mi_u_tr})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load results from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "mat_contents  = sio.loadmat(f'./saved_data/acc_celeba_{exp_info}.mat')\n",
    "util_acc_ts = mat_contents['util_acc_ts']\n",
    "util_acc_tr = mat_contents['util_acc_tr']\n",
    "util_acc_va = mat_contents['util_acc_va']\n",
    "sens_acc_tr = mat_contents['sens_acc_tr']\n",
    "sens_acc_ts = mat_contents['sens_acc_ts']\n",
    "sens_acc_tr_rev = mat_contents['sens_acc_tr_rev']\n",
    "sens_acc_ts_rev = mat_contents['sens_acc_ts_rev']\n",
    "sens_mae_ts = mat_contents['sens_mae_ts']\n",
    "sens_mae_tr = mat_contents['sens_mae_tr']\n",
    "sens_mae_ts_rev = mat_contents['sens_mae_ts_rev']\n",
    "sens_mae_tr_rev = mat_contents['sens_mae_tr_rev']\n",
    "\n",
    "\n",
    "mat_contents  = sio.loadmat(f'./saved_data/mi_celeba_{exp_info}.mat')\n",
    "mi_s_tr = mat_contents['mi_s_tr']\n",
    "mi_s_ts = mat_contents['mi_s_ts']\n",
    "mi_u_ts = mat_contents['mi_u_ts']\n",
    "mi_u_tr = mat_contents['mi_u_tr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_list = [0.0001, 0.1, 0.9]\n",
    "beta_list = [0.0001, 0.1, 0.3, 0.5, 0.8, 0.95]\n",
    "DIM_Z = [64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24",
   "language": "python",
   "name": "tf24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
